{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_regression_steepest_descent.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUjrF0-hhe6y",
        "outputId": "21fe2fb5-45ab-4cbd-b821-c7443c1be895"
      },
      "source": [
        "import random\r\n",
        "from sklearn import datasets\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import scipy.linalg as la\r\n",
        "import math\r\n",
        "from scipy.fftpack import fft,fftfreq\r\n",
        "from scipy.linalg import toeplitz\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#Newtons method ( steepest descent)\r\n",
        "def cost_derivative(thetain,X,Y,H):\r\n",
        "    y_hat=np.dot(X,thetain)\r\n",
        "    error=y_hat-Y\r\n",
        "    cost=np.dot(error.T,error)\r\n",
        "    thetaout=thetain-np.dot(H,2*np.dot(X.T,error))\r\n",
        "    return thetaout,cost\r\n",
        "df = pd.read_csv('assign2.csv')\r\n",
        "X=df.iloc[:,0:1]\r\n",
        "Y=df.iloc[:,1:2]\r\n",
        "X1=X.copy()\r\n",
        "meanx=X1.mean()\r\n",
        "stdx=X1.std()\r\n",
        "X1=np.array(X1)\r\n",
        "X1=X1.reshape(len(X1),)\r\n",
        "X = (X - X.mean()) / X.std()\r\n",
        "X=np.c_[ np.ones(X.shape[0]),X ]\r\n",
        "alpha=0.1\r\n",
        "Y=np.array(Y)\r\n",
        "Y=Y.reshape(len(Y),)\r\n",
        "theta=np.random.rand(2)\r\n",
        "old=theta.copy()\r\n",
        "oldc=0\r\n",
        "n=len(theta)\r\n",
        "z=np.zeros((n,n))\r\n",
        "for i in range(n):\r\n",
        "  for j in range(n):\r\n",
        "    z[i][j]=np.dot(X[:,i].T,X[:,j])\r\n",
        "z=np.linalg.inv(2*z)\r\n",
        "counter=0\r\n",
        "for i in range(100):\r\n",
        "    theta,cost=cost_derivative(old,X,Y,z)\r\n",
        "    counter=counter+1\r\n",
        "    if oldc<cost:\r\n",
        "      break\r\n",
        "    if i!=0 and oldc-cost==0:        \r\n",
        "      break\r\n",
        "    oldc=cost \r\n",
        "    old=theta.copy()        \r\n",
        "    \r\n",
        "theta[0]=theta[0]-theta[1]*meanx/stdx\r\n",
        "theta[1]=theta[1]/stdx\r\n",
        "print(\"theta values after iterations=\"+str(counter)+\"  for which we will obtain minimum of the loss function :theta_0=\" +str(theta[0])+\"  theta_1=\"+str(theta[1]))\r\n",
        "#Here loss function has two parameters theta0 and theta1,let theta=[theta0 theta1],so its Hessian matrix will be of dimensions 2x2.Its derivative and double derivative is calculated , we get Hij=2(X[:,i]^T*X[:,j]) where X[:,i] is the column vector ,here H will become constant for every theta.Here we get the exact theta for which the loss function will be minimum just after single iteration because here after differentiation of order more than 2 the function will become 0 so, here newtons method will directly gives us the theta same as the theta obtained by pseudo inverse method.If the function has nonzero derivative for orders more than 2 then it will take more than 1 iteration but here the 2nd order derivative will become constant.\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "theta values after iterations=1  for which we will obtain minimum of the loss function :theta_0=49.23762989433489  theta_1=-0.008611934783475307\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
